{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redaction in a PDF using Textract, Comprehend Medical, and muPDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by installing muPDF library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install PyMuPDF --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries will be using and set up initial variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import sagemaker\n",
    "from IPython.display import IFrame\n",
    "\n",
    "ON_SAGEMAKER_NOTEBOOK = True\n",
    "\n",
    "# Curent AWS Region. Use this to choose corresponding S3 bucket with sample content\n",
    "mySession = boto3.session.Session()\n",
    "awsRegion = mySession.region_name\n",
    "# Amazon S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Amazon Textract client\n",
    "textract = boto3.client('textract')\n",
    "\n",
    "if ON_SAGEMAKER_NOTEBOOK:\n",
    "    role = sagemaker.get_execution_role()\n",
    "else:\n",
    "    role = \"[YOUR ROLE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/sample_doctors_report.pdf'\n",
    "\n",
    "IFrame(filename, 900, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an S3 bucket\n",
    "And put your bucket name to replace: **your-s3-bucket-name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'your-s3-bucket-name'\n",
    "prefix = 'your-s3-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{prefix}/source_doc/{filename}\"\n",
    "doc_uri = f\"s3://{bucket}/{file_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {filename} {doc_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startJob(s3BucketName, objectName):\n",
    "    response = None\n",
    "    response = textract.start_document_analysis(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': objectName\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes = [\n",
    "        'FORMS', 'TABLES'\n",
    "    ],\n",
    "    )\n",
    "\n",
    "    return response[\"JobId\"]\n",
    "\n",
    "def isJobComplete(jobId):\n",
    "    response = textract.get_document_analysis(JobId=jobId)\n",
    "    status = response[\"JobStatus\"]\n",
    "    print(\"Job status: {}\".format(status))\n",
    "\n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        time.sleep(5)\n",
    "        response = textract.get_document_analysis(JobId=jobId)\n",
    "        status = response[\"JobStatus\"]\n",
    "        print(\"Job status: {}\".format(status))\n",
    "\n",
    "    return status\n",
    "\n",
    "def getJobResults(jobId):\n",
    "\n",
    "    pages = []\n",
    "    response = textract.get_document_analysis(JobId=jobId)\n",
    "    \n",
    "    pages.append(response)\n",
    "    print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "\n",
    "    while(nextToken):\n",
    "        response = textract.get_document_analysis(JobId=jobId, NextToken=nextToken)\n",
    "\n",
    "        pages.append(response)\n",
    "        print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "        nextToken = None\n",
    "        if('NextToken' in response):\n",
    "            nextToken = response['NextToken']\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobId = startJob(bucket, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(isJobComplete(jobId)):\n",
    "    textract_response = getJobResults(jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textract_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"txtract_output.json\", \"w\") as txtract_output:\n",
    "    json.dump(textract_response, txtract_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write detected text\n",
    "txtbuf = \"\"\n",
    "with open(\"textract_text.txt\", \"w\") as wfd:\n",
    "    for resultPage in textract_response:\n",
    "        for item in resultPage[\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                wfd.write(item[\"Text\"] + ' ')\n",
    "                txtbuf += item[\"Text\"] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textract_txt_uri = f\"s3://{bucket}/{prefix}/textract_txt/textract_text.txt\"\n",
    "! aws s3 cp \"textract_text.txt\" {textract_txt_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client(service_name='comprehendmedical', region_name=awsRegion)\n",
    "\n",
    "# setup input and output and job id\n",
    "import uuid\n",
    "\n",
    "job_uuid = uuid.uuid1()\n",
    "job_name = f\"comprehend-medical-job-{job_uuid}\"\n",
    "print(\"job_name = \"+job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### myDataAccessRole\n",
    "Replace the string **your-data-access-role-arn** with the ARN of the Role you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.start_entities_detection_v2_job(\n",
    "    InputDataConfig={\n",
    "        'S3Bucket': bucket,\n",
    "        'S3Key': f'{prefix}/textract_txt'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Bucket': bucket,\n",
    "        'S3Key': f'{prefix}/results'\n",
    "    },\n",
    "    DataAccessRoleArn = \"your-data-access-role-arn\",\n",
    "    JobName=job_name,\n",
    "    LanguageCode='en',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the job ID\n",
    "events_job_id = response['JobId']\n",
    "print(\"events_job_id = \"+events_job_id)\n",
    "job = comprehend.describe_entities_detection_v2_job(JobId=events_job_id)\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using datetime module\n",
    "import datetime;\n",
    "from time import sleep\n",
    "\n",
    "while True:\n",
    "    job = comprehend.describe_entities_detection_v2_job(JobId=events_job_id)\n",
    "    status = job['ComprehendMedicalAsyncJobProperties']['JobStatus']\n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "    sleep(10)\n",
    "    # ct stores current time\n",
    "    ct = datetime.datetime.now()\n",
    "    print(\"-- still processing --> \" + str(ct))\n",
    "print(\"-- done --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bucket = job['ComprehendMedicalAsyncJobProperties']['OutputDataConfig']['S3Bucket']\n",
    "res_key = job['ComprehendMedicalAsyncJobProperties']['OutputDataConfig']['S3Key']\n",
    "res_file = f'{res_key}textract_text.txt.out'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the Comprehend pii output file to process locally\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(res_bucket, res_file, 'compmed_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('compmed_output.json', \"r\") as rfd:\n",
    "    wom = json.load(rfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_list = []\n",
    "for f in wom['Entities']:\n",
    "    if f['Category'] == 'ANATOMY':\n",
    "        text = txtbuf[f['BeginOffset']:f['EndOffset']]\n",
    "        if text not in red_list:\n",
    "            red_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"txtract_output.json\", \"r\") as txtract_output:\n",
    "    response = json.load(txtract_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultPage = response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_items = []\n",
    "for resultPage in textract_response:\n",
    "    for item in resultPage[\"Blocks\"]:\n",
    "        if item['BlockType'] == 'WORD':\n",
    "            if item['Text'] in red_list:\n",
    "               # print(item['Text'])\n",
    "                bbox_items.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(filename)\n",
    "debug = False\n",
    "\n",
    "# pixels per inch\n",
    "ppi = 72\n",
    "\n",
    "# padding for the highlight for the PII annotations\n",
    "pad = 1\n",
    "\n",
    "# the units for Rect are in pixels\n",
    "\n",
    "for item in bbox_items:\n",
    "    page = doc[item['Page']-1]\n",
    "    \n",
    "    # Get page mediabox size in pixels\n",
    "    mediabox_width, mediabox_height = page.mediabox_size\n",
    "\n",
    "    page_height = mediabox_height / ppi\n",
    "    page_width = mediabox_width / ppi\n",
    "    \n",
    "    text = item['Text']\n",
    "    bbox = item['Geometry']['BoundingBox']\n",
    "\n",
    "    # note: each of the bbox values is a ratio of the overall document page height or width\n",
    "    top = bbox['Top'] * page_height\n",
    "    left = bbox['Left'] * page_width\n",
    "    bottom = top + (bbox['Height'] * page_height)\n",
    "    right = left + (bbox['Width'] * page_width)\n",
    "\n",
    "    rect = fitz.Rect(left*ppi-pad, top*ppi-pad, right*ppi+pad, bottom*ppi+pad)\n",
    "\n",
    "    red = (1, 0, 0)\n",
    "    annot = page.add_rect_annot(rect)\n",
    "    #annot.set_border(width=1, dashes=[1, 2])\n",
    "    annot.set_colors(stroke=red, fill=red)\n",
    "    annot.update(opacity=1)\n",
    "\n",
    "doc.save('output.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redacted_filename = 'output.pdf'\n",
    "IFrame(redacted_filename, 900, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
